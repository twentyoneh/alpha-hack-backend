run LLM local:


docker run -ti --name local-ai -p 8080:8080 localai/localai:latest run llama-3.2-1b-instruct:q4_k_m

http://localhost:8080/